# Basic-Crawler
This is a demonstration of a basic crawler using python and considers robots.txt

Remember that robots.txt is a guideline, not a strict security measure. Some websites might choose not to enforce it, while others rely heavily on it. 

This file, which should be stored at the document root of every web server, contains various directives and parameters which instruct bots, spiders, and crawlers what they can and cannot view.

If your web crawler doesn't comply with the rules in the robots.txt file, it could have the following impacts:

 a. Ignoring robots.txt rules could lead to legal issues and ethical concerns
 
 b. Websites may block or ban IP addresses that ignore their robots.txt rules. 
 
 c. Crawling websites without respecting the crawl rate specified in the robots.txt file can put unnecessary load on the server

Pre-requisites : 

1. **pip install requests**

2. **pip install beautifulsoup4**
